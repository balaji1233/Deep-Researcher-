{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEOl_rJsa3M7"
      },
      "outputs": [],
      "source": [
        "ollama pull llama3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "\n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response['results']\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and 'results' in response:\n",
        "                sources_list.extend(response['results'])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
        "\n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source['url'] not in unique_sources:\n",
        "            unique_sources[source['url']] = source\n",
        "\n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get('raw_content', '')\n",
        "            if raw_content is None:\n",
        "                raw_content = ''\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "\n",
        "    return formatted_text.strip()\n",
        "\n",
        "def format_sources(search_results):\n",
        "    \"\"\"Format search results into a bullet-point list of sources.\n",
        "\n",
        "    Args:\n",
        "        search_results (dict): Tavily search response containing results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with sources and their URLs\n",
        "    \"\"\"\n",
        "    return '\\n'.join(\n",
        "        f\"* {source['title']} : {source['url']}\"\n",
        "        for source in search_results['results']\n",
        "    )\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query, include_raw_content=True, max_results=3):\n",
        "    \"\"\" Search the web using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "        include_raw_content (bool): Whether to include the raw_content from Tavily in the formatted string\n",
        "        max_results (int): Maximum number of results to return\n",
        "\n",
        "    Returns:\n",
        "        dict: Tavily search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "\n",
        "    return tavily_client.search(query,\n",
        "                         max_results=max_results,\n",
        "                         include_raw_content=include_raw_content)"
      ],
      "metadata": {
        "id": "g6HmsT9CbFR7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "id": "wRjtvYR4cZ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "### LLM\n",
        "local_llm = \"llama3.2\"\n",
        "local_llm = \"qwen2.5:14b\"\n",
        "llm = ChatOllama(model=local_llm, temperature=0)\n",
        "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
        "\n",
        "### Search\n",
        "from tavily import TavilyClient\n",
        "tavily_client = TavilyClient()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fsWXJH7ibJD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from dataclasses import dataclass, field\n",
        "from typing_extensions import TypedDict, Annotated, Literal\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryState:\n",
        "    research_topic: str = field(default=None) # Report topic\n",
        "    search_query: str = field(default=None) # Search query\n",
        "    web_research_results: Annotated[list, operator.add] = field(default_factory=list)\n",
        "    sources_gathered: Annotated[list, operator.add] = field(default_factory=list)\n",
        "    research_loop_count: int = field(default=0) # Research loop count\n",
        "    running_summary: str = field(default=None) # Final report\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryStateInput(TypedDict):\n",
        "    research_topic: str = field(default=None) # Report topic\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryStateOutput(TypedDict):\n",
        "    running_summary: str = field(default=None) # Final report"
      ],
      "metadata": {
        "id": "vqM8yRPSbKBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from configuration import Configuration\n",
        "\n",
        "\n",
        "### Query Writer\n",
        "query_writer_instructions=\"\"\"Your goal is to generate targeted web search query.\n",
        "\n",
        "The query will gather information related to a specific topic.\n",
        "\n",
        "Topic:\n",
        "{research_topic}\n",
        "\n",
        "Return your query as a JSON object:\n",
        "{{\n",
        "    \"query\": \"string\",\n",
        "    \"aspect\": \"string\",\n",
        "    \"rationale\": \"string\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "summarizer_instructions=\"\"\"Your goal is to generate a high-quality summary of the web search results.\n",
        "\n",
        "When EXTENDING an existing summary:\n",
        "1. Seamlessly integrate new information without repeating what's already covered\n",
        "2. Maintain consistency with the existing content's style and depth\n",
        "3. Only add new, non-redundant information\n",
        "4. Ensure smooth transitions between existing and new content\n",
        "\n",
        "When creating a NEW summary:\n",
        "1. Highlight the most relevant information from each source\n",
        "2. Provide a concise overview of the key points related to the report topic\n",
        "3. Emphasize significant findings or insights\n",
        "4. Ensure a coherent flow of information\n",
        "\n",
        "In both cases:\n",
        "- Focus on factual, objective information\n",
        "- Maintain a consistent technical depth\n",
        "- Avoid redundancy and repetition\n",
        "- DO NOT use phrases like \"based on the new results\" or \"according to additional sources\"\n",
        "- DO NOT add a preamble like \"Here is an extended summary ...\" Just directly output the summary.\n",
        "- DO NOT add a References or Works Cited section.\n",
        "\"\"\"\n",
        "\n",
        "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
        "\n",
        "Your tasks:\n",
        "1. Identify knowledge gaps or areas that need deeper exploration\n",
        "2. Generate a follow-up question that would help expand your understanding\n",
        "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
        "\n",
        "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
        "\n",
        "Return your analysis as a JSON object:\n",
        "{{\n",
        "    \"knowledge_gap\": \"string\",\n",
        "    \"follow_up_query\": \"string\"\n",
        "}}\"\"\"\n",
        "\n",
        "def generate_query(state: SummaryState):\n",
        "    \"\"\" Generate a query for web search \"\"\"\n",
        "\n",
        "    # Format the prompt\n",
        "    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic)\n",
        "\n",
        "    # Generate a query\n",
        "    result = llm_json_mode.invoke(\n",
        "        [SystemMessage(content=query_writer_instructions_formatted),\n",
        "        HumanMessage(content=f\"Generate a query for web search:\")]\n",
        "    )\n",
        "    query = json.loads(result.content)\n",
        "\n",
        "    return {\"search_query\": query['query']}\n",
        "\n",
        "def web_research(state: SummaryState):\n",
        "    \"\"\" Gather information from the web \"\"\"\n",
        "\n",
        "    # Search the web\n",
        "    search_results = tavily_search(state.search_query, include_raw_content=True, max_results=1)\n",
        "    # Format the sources\n",
        "    search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000)\n",
        "    return {\"sources_gathered\": [format_sources(search_results)], \"research_loop_count\": state.research_loop_count + 1, \"web_research_results\": [search_str]}\n",
        "\n",
        "def summarize_sources(state: SummaryState):\n",
        "    \"\"\" Summarize the gathered sources \"\"\"\n",
        "\n",
        "    # Existing summary\n",
        "    existing_summary = state.running_summary\n",
        "\n",
        "    # Most recent web research\n",
        "    most_recent_web_research = state.web_research_results[-1]\n",
        "\n",
        "    # Build the human message\n",
        "    if existing_summary:\n",
        "        human_message_content = (\n",
        "            f\"Extend the existing summary: {existing_summary}\\n\\n\"\n",
        "            f\"Include new search results: {most_recent_web_research} \"\n",
        "            f\"That addresses the following topic: {state.research_topic}\"\n",
        "        )\n",
        "    else:\n",
        "        human_message_content = (\n",
        "            f\"Generate a summary of these search results: {most_recent_web_research} \"\n",
        "            f\"That addresses the following topic: {state.research_topic}\"\n",
        "        )\n",
        "\n",
        "    # Run the LLM\n",
        "    result = llm.invoke(\n",
        "        [SystemMessage(content=summarizer_instructions),\n",
        "        HumanMessage(content=human_message_content)]\n",
        "    )\n",
        "\n",
        "    running_summary = result.content\n",
        "    return {\"running_summary\": running_summary}\n",
        "\n",
        "def reflect_on_summary(state: SummaryState):\n",
        "    \"\"\" Reflect on the summary and generate a follow-up query \"\"\"\n",
        "\n",
        "    # Generate a query\n",
        "    result = llm_json_mode.invoke(\n",
        "        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic)),\n",
        "        HumanMessage(content=f\"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}\")]\n",
        "    )\n",
        "    follow_up_query = json.loads(result.content)\n",
        "\n",
        "    # Overwrite the search query\n",
        "    return {\"search_query\": follow_up_query['follow_up_query']}\n",
        "\n",
        "def finalize_summary(state: SummaryState):\n",
        "    \"\"\" Finalize the summary \"\"\"\n",
        "\n",
        "    # Format all accumulated sources into a single bulleted list\n",
        "    all_sources = \"\\n\".join(source for source in state.sources_gathered)\n",
        "    state.running_summary = f\"## Summary\\n\\n{state.running_summary}\\n\\n ### Sources:\\n{all_sources}\"\n",
        "    return {\"running_summary\": state.running_summary}\n",
        "\n",
        "def route_research(state: SummaryState, config: RunnableConfig) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
        "    \"\"\" Route the research based on the follow-up query \"\"\"\n",
        "\n",
        "    configurable = Configuration.from_runnable_config(config)\n",
        "    if state.research_loop_count <= configurable.max_web_research_loops:\n",
        "        return \"web_research\"\n",
        "    else:\n",
        "        return \"finalize_summary\"\n",
        "\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(SummaryState, input=SummaryStateInput, output=SummaryStateOutput, config_schema=Configuration)\n",
        "builder.add_node(\"generate_query\", generate_query)\n",
        "builder.add_node(\"web_research\", web_research)\n",
        "builder.add_node(\"summarize_sources\", summarize_sources)\n",
        "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
        "builder.add_node(\"finalize_summary\", finalize_summary)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"generate_query\")\n",
        "builder.add_edge(\"generate_query\", \"web_research\")\n",
        "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
        "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
        "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
        "builder.add_edge(\"finalize_summary\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "nHpN620qbQEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "research_input = SummaryStateInput(\n",
        "    research_topic=\"Overview of training for \"\n",
        ")\n",
        "summary = graph.invoke(research_input)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X0IDAQq5bRGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}